"""
GNN-SubNet ì „ì²´ íŒŒì´í”„ë¼ì¸

ë…¼ë¬¸ ë°©ì‹:
1. Graph ë°ì´í„° ìƒì„± (PPI + Proteomics)
2. GIN ëª¨ë¸ í•™ìŠµ
3. Model-wide explanations
4. Disease subnetwork detection
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import random
import pickle
from pathlib import Path
from datetime import datetime
from torch_geometric.loader import DataLoader
from torch_geometric.data import Batch
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, accuracy_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns

from src.data.graph_dataset import GraphDataLoader
from src.view_gnn.models.graph_classifier import GraphClassifier
# Explain ê´€ë ¨ importëŠ” explain_gnn_subnet.pyë¡œ ì´ë™


def train_epoch(model, loader, optimizer, criterion, device):
    """í•œ epoch í•™ìŠµ (BCEWithLogitsLoss ì ìš©)"""
    model.train()
    total_loss = 0
    
    all_preds = []
    all_probs = []
    all_labels = []
    
    for batch in loader:
        batch = batch.to(device)
        
        # Forward
        logits = model(batch.x, batch.edge_index, batch.batch)
        
        # [ìˆ˜ì •] ì°¨ì› ë§ì¶”ê¸° (BCELossëŠ” [N] í˜•íƒœì˜ float ì…ë ¥ì„ ì›í•¨)
        if logits.shape[1] == 2:
            # Class 1(Dementia)ì— ëŒ€í•œ Logitë§Œ ê°€ì ¸ì˜´
            out = logits[:, 1]
        else:
            out = logits.squeeze()
            
        # Labelì„ floatìœ¼ë¡œ ë³€í™˜ í•„ìˆ˜
        loss = criterion(out, batch.y.float())
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        
        # Gradient clipping (ì™„í™”: 1.0 -> 5.0, í•™ìŠµ ì´‰ì§„)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
        
        optimizer.step()
        
        # Stats accumulation
        total_loss += loss.item()
        
        # [ìˆ˜ì •] í™•ë¥  ë° ì˜ˆì¸¡ ê³„ì‚° (Sigmoid ì‚¬ìš©)
        probs = torch.sigmoid(out)
        threshold = 0.1  # 0.3 -> 0.1 (ê·¹ì‹¬í•œ ë¶ˆê· í˜• ëŒ€ì‘, ë” ê³µê²©ì )
        preds = (probs > threshold).long()  # threshold ë„˜ìœ¼ë©´ ì¹˜ë§¤
        
        all_preds.extend(preds.cpu().detach().numpy())
        all_probs.extend(probs.cpu().detach().numpy())
        all_labels.extend(batch.y.cpu().detach().numpy())
    
    # Calculate Metrics
    all_preds = np.array(all_preds)
    all_probs = np.array(all_probs)
    all_labels = np.array(all_labels)
    
    try:
        acc = accuracy_score(all_labels, all_preds)
        auc = roc_auc_score(all_labels, all_probs)
        f1 = f1_score(all_labels, all_preds, zero_division=0)
    except:
        acc, auc, f1 = 0.0, 0.5, 0.0
    
    return total_loss / len(loader), acc, auc, f1

def evaluate(model, loader, criterion, device):
    """í‰ê°€ (BCEWithLogitsLoss ì ìš©)"""
    model.eval()
    total_loss = 0
    all_preds = []
    all_probs = []
    all_labels = []
    
    with torch.no_grad():
        for batch in loader:
            batch = batch.to(device)
            
            logits = model(batch.x, batch.edge_index, batch.batch)
            
            # [ìˆ˜ì •] ì°¨ì› ë§ì¶”ê¸°
            if logits.shape[1] == 2:
                out = logits[:, 1]
            else:
                out = logits.squeeze()
                
            loss = criterion(out, batch.y.float())
            
            total_loss += loss.item()
            
            # [ìˆ˜ì •] í™•ë¥  ê³„ì‚°
            probs = torch.sigmoid(out)
            
            # Thresholdë¥¼ í´ë˜ìŠ¤ ë¹„ìœ¨ì— ë§ì¶° ì¡°ì • (30:1 ë¹„ìœ¨ì´ë©´ thresholdë¥¼ ë§¤ìš° ë‚®ì¶¤)
            # ê·¹ì‹¬í•œ ë¶ˆê· í˜•ì—ì„œëŠ” thresholdë¥¼ ë‚®ì¶°ì„œ ë” ë§ì€ ì–‘ì„± ì˜ˆì¸¡ í—ˆìš©
            threshold = 0.1  # 0.3 -> 0.1 (ë” ê³µê²©ì ìœ¼ë¡œ ì–‘ì„± ì˜ˆì¸¡)
            preds = (probs > threshold).long()
            
            all_preds.extend(preds.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
            all_labels.extend(batch.y.cpu().numpy())
    
    all_preds = np.array(all_preds)
    all_probs = np.array(all_probs)
    all_labels = np.array(all_labels)
    
    # ì˜ˆì¸¡ ë¶„í¬ í™•ì¸ (ë””ë²„ê¹…ìš©)
    n_pred_0 = (all_preds == 0).sum()
    n_pred_1 = (all_preds == 1).sum()
    n_label_0 = (all_labels == 0).sum()
    n_label_1 = (all_labels == 1).sum()
    
    acc = accuracy_score(all_labels, all_preds)
    try:
        auc = roc_auc_score(all_labels, all_probs)
        f1 = f1_score(all_labels, all_preds, zero_division=0)
    except:
        auc = 0.5
        f1 = 0.0
    
    # í´ë˜ìŠ¤ë³„ í™•ë¥  ë¶„í¬ ë¶„ì„ (AUCê°€ ë†’ì€ ì´ìœ  í™•ì¸)
    probs_class_0 = all_probs[all_labels == 0]  # Control ê·¸ë£¹ì˜ í™•ë¥ 
    probs_class_1 = all_probs[all_labels == 1]  # Dementia ê·¸ë£¹ì˜ í™•ë¥ 
    
    # ì˜ˆì¸¡ ë¶„í¬ ì¶œë ¥ (F1ì´ ë‚®ê±°ë‚˜ ì˜ˆì¸¡ì´ í¸í–¥ë  ë•Œë§Œ ìƒì„¸ ì¶œë ¥)
    if f1 < 0.2 or n_pred_1 < len(all_preds) * 0.05:  # F1ì´ ë‚®ê±°ë‚˜ ì–‘ì„± ì˜ˆì¸¡ì´ 5% ë¯¸ë§Œì¼ ë•Œ
        print(f"      âš ï¸  ì˜ˆì¸¡ ë¶„í¬: Pred 0={n_pred_0:,} ({n_pred_0/len(all_preds)*100:.1f}%), Pred 1={n_pred_1:,} ({n_pred_1/len(all_preds)*100:.1f}%)")
        print(f"      âš ï¸  ì‹¤ì œ ë¶„í¬: Label 0={n_label_0:,} ({n_label_0/len(all_labels)*100:.1f}%), Label 1={n_label_1:,} ({n_label_1/len(all_labels)*100:.1f}%)")
        print(f"      âš ï¸  ì „ì²´ í™•ë¥  í†µê³„: min={all_probs.min():.3f}, max={all_probs.max():.3f}, mean={all_probs.mean():.3f}, median={np.median(all_probs):.3f}")
        print(f"      ğŸ“Š Control ê·¸ë£¹ í™•ë¥ : mean={probs_class_0.mean():.3f}, median={np.median(probs_class_0):.3f}, std={probs_class_0.std():.3f}")
        print(f"      ğŸ“Š Dementia ê·¸ë£¹ í™•ë¥ : mean={probs_class_1.mean():.3f}, median={np.median(probs_class_1):.3f}, std={probs_class_1.std():.3f}")
        
        # AUCê°€ ë†’ì€ ì´ìœ  ë¶„ì„
        if len(probs_class_0) > 0 and len(probs_class_1) > 0:
            prob_diff = probs_class_1.mean() - probs_class_0.mean()
            print(f"      ğŸ” AUC ë¶„ì„: Dementia í‰ê·  í™•ë¥ ì´ Controlë³´ë‹¤ {prob_diff:+.3f} ë†’ìŒ")
            if prob_diff > 0.05:
                print(f"      âœ… ëª¨ë¸ì´ í™•ë¥  ìˆœìœ„ëŠ” ì˜ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤ (AUC={auc:.3f})")
                print(f"      âŒ í•˜ì§€ë§Œ threshold={threshold:.2f}ê°€ ë„ˆë¬´ ë†’ì•„ì„œ F1ì´ ë‚®ìŠµë‹ˆë‹¤")
                print(f"      ğŸ’¡ í•´ê²°ì±…: thresholdë¥¼ {probs_class_1.mean():.2f} ê·¼ì²˜ë¡œ ì¡°ì •í•˜ë©´ F1ì´ ê°œì„ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            else:
                print(f"      âš ï¸  ëª¨ë¸ì´ ë‘ ê·¸ë£¹ì„ ê±°ì˜ êµ¬ë¶„í•˜ì§€ ëª»í•˜ê³  ìˆìŠµë‹ˆë‹¤!")
                print(f"      âš ï¸  AUCê°€ ë†’ì€ ê²ƒì€ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë•Œë¬¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        print(f"      ğŸ’¡ í•´ê²°ì±…: pos_weight ì¦ê°€ ë˜ëŠ” threshold=0.1 ì´í•˜ë¡œ ì¡°ì • í•„ìš”")
    
    return total_loss / len(loader), acc, auc, f1

def main():
    print("=" * 70)
    print("GNN-SubNet: Disease Subnetwork Detection")
    print("=" * 70)
    
    # ëª¨ë¸ ì €ì¥ ê²½ë¡œì— ë‚ ì§œ ì¶”ê°€
    date_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_path = f'../../data/gnn_subnet_best_model_{date_str}.pt'
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\n[1/5] ë°ì´í„° ë¡œë“œ...")
    master_file = "../../data/ukb/ukb_usable_master.parquet"
    df = pd.read_parquet(master_file)
    
    # ë‹¨ë°±ì§ˆ ì»¬ëŸ¼ ì¶”ì¶œ
    exclude_cols = ["eid", "sex", "target_age", "target_dementia", "participant.p42018"]
    protein_cols = [c for c in df.columns 
                   if c not in exclude_cols 
                   and not c.startswith("pc__")
                   and not c.startswith("assess__")
                   and not c.startswith("online__")]
    
    print(f"   - ìƒ˜í”Œ ìˆ˜: {len(df):,}")
    print(f"   - ë‹¨ë°±ì§ˆ ìˆ˜: {len(protein_cols):,}")
    print(f"   - Dementia: {(df['target_dementia'] == 1).sum():,}")
    print(f"   - Control: {(df['target_dementia'] == 0).sum():,}")
    
    # [í•µì‹¬ ìˆ˜ì • 1] ë°ì´í„° ì •ê·œí™” (Standard Scaling) - í•„ìˆ˜!
    from sklearn.preprocessing import StandardScaler
    print("\n   âš ï¸  ë°ì´í„° ì •ê·œí™” (Standard Scaling) ì ìš© ì¤‘... (í•„ìˆ˜!)")
    scaler = StandardScaler()
    # ë‹¨ë°±ì§ˆ ì»¬ëŸ¼ë§Œ ê³¨ë¼ì„œ ìŠ¤ì¼€ì¼ë§ (í‰ê·  0, ë¶„ì‚° 1ë¡œ ë³€í™˜)
    df[protein_cols] = scaler.fit_transform(df[protein_cols])
    print("   âœ… ì •ê·œí™” ì™„ë£Œ (í‰ê· =0, ë¶„ì‚°=1)")
    
    # 2. Graph ë°ì´í„° ìƒì„±
    print("\n[2/5] Graph ë°ì´í„° ìƒì„±...")
    
    # [ìµœì¢… íŠœë‹] ê·¸ë˜í”„ ì—°ê²°ì„± ê°œì„ : Threshold ì™„í™”
    score_threshold = 700  # 800 -> 700 (High Confidence, ì—°ê²°ì„± ìµœëŒ€í™”)
    loader = GraphDataLoader(score_threshold=score_threshold)
    print(f"   - PPI Score Threshold: {score_threshold} (ê·¸ë˜í”„ ê²½ëŸ‰í™”)")
    
    # ---------------------------------------------------------
    # [ìˆ˜ì •] ì „ì²´ ë°ì´í„°(53,000ëª…) ëª¨ë‘ ì‚¬ìš©!
    # ---------------------------------------------------------
    # ìƒ˜í”Œë§ ì—†ì´ ì „ì²´ ì‚¬ìš©
    df_sample = df.copy().sample(frac=1, random_state=42).reset_index(drop=True)
    
    # í´ë˜ìŠ¤ ê°œìˆ˜ í™•ì¸
    n_pos = (df_sample['target_dementia'] == 1).sum()
    n_neg = (df_sample['target_dementia'] == 0).sum()
    class_ratio = n_neg / n_pos if n_pos > 0 else 1.0
    
    print(f"   ğŸ”¥ Full Dataset Mode On!")
    print(f"     > Dementia: {n_pos:,}ëª…")
    print(f"     > Control:  {n_neg:,}ëª…")
    print(f"     > Total:    {len(df_sample):,}ëª…")
    print(f"     > ì‹¤ì œ ë¹„ìœ¨: 1 : {class_ratio:.1f}")
    
    # ê·¸ë˜í”„ ë°ì´í„°ì…‹ ì €ì¥ ê²½ë¡œ
    dataset_dir = Path("../../data/gnn")
    dataset_dir.mkdir(parents=True, exist_ok=True)
    dataset_file = dataset_dir / "graph_dataset_full.pkl"
    
    # ì €ì¥ëœ ë°ì´í„°ì…‹ì´ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒì„± í›„ ì €ì¥
    if dataset_file.exists():
        print(f"\n   ğŸ“‚ ì €ì¥ëœ ê·¸ë˜í”„ ë°ì´í„°ì…‹ ë°œê²¬: {dataset_file}")
        print(f"   âš¡ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
        with open(dataset_file, 'rb') as f:
            graphs = pickle.load(f)
        print(f"   âœ… ê·¸ë˜í”„ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(graphs):,}ê°œ")
    else:
        print(f"\n   ğŸ”¨ ê·¸ë˜í”„ ë°ì´í„°ì…‹ ìƒì„± ì¤‘... (ì²˜ìŒ ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤)")
        graphs = loader.create_graph_dataset(
            df_sample,
            protein_cols,
            label_col='target_dementia',
            eid_col='eid'
        )
        print(f"   - ìƒì„±ëœ ê·¸ë˜í”„: {len(graphs):,}ê°œ")
        
        # ë°ì´í„°ì…‹ ì €ì¥
        print(f"   ğŸ’¾ ê·¸ë˜í”„ ë°ì´í„°ì…‹ ì €ì¥ ì¤‘: {dataset_file}")
        with open(dataset_file, 'wb') as f:
            pickle.dump(graphs, f)
        print(f"   âœ… ì €ì¥ ì™„ë£Œ! ë‹¤ìŒ ì‹¤í–‰ë¶€í„°ëŠ” ìë™ìœ¼ë¡œ ë¡œë“œë©ë‹ˆë‹¤.")
    
    # 3. Train/Test split (Downsampling ì œê±° - Weighted Loss ì‚¬ìš©)
    print("\n[3/5] Train/Test split (ì „ì²´ ë°ì´í„° ì‚¬ìš©, Weighted Loss ì ìš©)...")
    
    # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
    labels = [g.y.item() for g in graphs]
    n_class_0 = labels.count(0)
    n_class_1 = labels.count(1)
    
    print(f"   - Class 0 (Control): {n_class_0:,}ê°œ")
    print(f"   - Class 1 (Dementia): {n_class_1:,}ê°œ")
    print(f"   - í´ë˜ìŠ¤ ë¹„ìœ¨: {n_class_0/n_class_1:.2f}:1")
    print(f"   âš ï¸  Downsampling ì—†ì´ ì „ì²´ {len(graphs):,}ê°œ ë°ì´í„° ì‚¬ìš©")
    print(f"   âœ… Weighted Lossë¡œ í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ ì˜ˆì •")
    
    # Train/Val/Test split (ì „ì²´ ë°ì´í„° ì‚¬ìš©, í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€)
    # 1ë‹¨ê³„: Train + (Val + Test)ë¡œ ë¶„ë¦¬
    train_graphs, temp_graphs = train_test_split(
        graphs,
        test_size=0.2,  # Val + Test = 20%
        random_state=42,
        stratify=labels
    )
    
    # 2ë‹¨ê³„: Valê³¼ Testë¡œ ë¶„ë¦¬ (tempì˜ 50%ì”©)
    temp_labels = [g.y.item() for g in temp_graphs]
    val_graphs_raw, test_graphs_raw = train_test_split(
        temp_graphs,
        test_size=0.5,  # Val 10%, Test 10%
        random_state=42,
        stratify=temp_labels
    )
    
    # 3ë‹¨ê³„: Valê³¼ Testë¥¼ ê· í˜• ë°ì´í„°ë¡œ ë§Œë“¤ê¸° (1:1 ë¹„ìœ¨)
    # Val ê· í˜•í™”
    val_labels = [g.y.item() for g in val_graphs_raw]
    val_class_0_raw = [g for g in val_graphs_raw if g.y.item() == 0]
    val_class_1_raw = [g for g in val_graphs_raw if g.y.item() == 1]
    n_val_class_1 = len(val_class_1_raw)
    # Controlì„ Dementia ê°œìˆ˜ë§Œí¼ë§Œ ìƒ˜í”Œë§
    val_class_0_balanced = random.sample(val_class_0_raw, min(n_val_class_1, len(val_class_0_raw)))
    val_graphs = val_class_0_balanced + val_class_1_raw
    random.shuffle(val_graphs)
    
    # Test ê· í˜•í™”
    test_labels = [g.y.item() for g in test_graphs_raw]
    test_class_0_raw = [g for g in test_graphs_raw if g.y.item() == 0]
    test_class_1_raw = [g for g in test_graphs_raw if g.y.item() == 1]
    n_test_class_1 = len(test_class_1_raw)
    # Controlì„ Dementia ê°œìˆ˜ë§Œí¼ë§Œ ìƒ˜í”Œë§
    test_class_0_balanced = random.sample(test_class_0_raw, min(n_test_class_1, len(test_class_0_raw)))
    test_graphs = test_class_0_balanced + test_class_1_raw
    random.shuffle(test_graphs)
    
    print(f"   - Train: {len(train_graphs):,}ê°œ (80%, ë¶ˆê· í˜• ìœ ì§€)")
    print(f"   - Val:   {len(val_graphs):,}ê°œ (ê· í˜• ë°ì´í„°, 1:1)")
    print(f"   - Test:  {len(test_graphs):,}ê°œ (ê· í˜• ë°ì´í„°, 1:1)")
    
    # Train set í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
    train_class_0 = sum(1 for g in train_graphs if g.y.item() == 0)
    train_class_1 = sum(1 for g in train_graphs if g.y.item() == 1)
    train_ratio = train_class_0 / train_class_1 if train_class_1 > 0 else 0.0
    print(f"   - Train Class 0: {train_class_0:,}, Train Class 1: {train_class_1:,}")
    print(f"   - Train ë¹„ìœ¨: 1:{train_ratio:.2f} (ë¶ˆê· í˜• ìœ ì§€ - í•™ìŠµìš©)")
    
    # Val set í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
    val_class_0 = sum(1 for g in val_graphs if g.y.item() == 0)
    val_class_1 = sum(1 for g in val_graphs if g.y.item() == 1)
    val_ratio = val_class_0 / val_class_1 if val_class_1 > 0 else 0.0
    print(f"   - Val Class 0: {val_class_0:,}, Val Class 1: {val_class_1:,}")
    print(f"   - Val ë¹„ìœ¨: 1:{val_ratio:.2f} (ê· í˜• - í‰ê°€ìš©)")
    
    # Test set í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
    test_class_0 = sum(1 for g in test_graphs if g.y.item() == 0)
    test_class_1 = sum(1 for g in test_graphs if g.y.item() == 1)
    test_ratio = test_class_0 / test_class_1 if test_class_1 > 0 else 0.0
    print(f"   - Test Class 0: {test_class_0:,}, Test Class 1: {test_class_1:,}")
    print(f"   - Test ë¹„ìœ¨: 1:{test_ratio:.2f} (ê· í˜• - í‰ê°€ìš©)")
    
    print(f"   âœ… Trainì€ ë¶ˆê· í˜• ë°ì´í„°ë¡œ í•™ìŠµ, Val/TestëŠ” ê· í˜• ë°ì´í„°ë¡œ í‰ê°€")
    
    # 4. ëª¨ë¸ í•™ìŠµ
    print("\n[4/5] ëª¨ë¸ í•™ìŠµ...")
    # GPU ì„¤ì •
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"   - Device: {device}")
        print(f"   - GPU: {torch.cuda.get_device_name(0)}")
        print(f"   - CUDA Version: {torch.version.cuda}")
    else:
        device = torch.device('cpu')
        print(f"   - Device: {device} (âš ï¸  GPU not available, using CPU)")
    
    # [GPU í™œìš©ë„ ê°œì„ ] ëª¨ë¸ í¬ê¸° ì¡°ì • (Over-smoothing ë°©ì§€ + GPU í™œìš© ê· í˜•)
    # LayersëŠ” 2 ìœ ì§€ (Over-smoothing ë°©ì§€), Hidden/Embeddingì€ ì¦ê°€ (GPU í™œìš©)
    model = GraphClassifier(
        input_dim=1,
        hidden_dim=64,      # 32 -> 64 (GPU í™œìš©ë„ ì¦ê°€)
        embedding_dim=256,  # 128 -> 256 (GPU í™œìš©ë„ ì¦ê°€)
        num_layers=2,       # 2 ìœ ì§€ (Over-smoothing ë°©ì§€)
        num_classes=2,
        pooling='mean',
        dropout=0.2         # 0.2 ìœ ì§€ (ê·œì œ)
    ).to(device)
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    model_size_mb = total_params * 4 / 1024 / 1024  # FP32 ê¸°ì¤€
    
    print(f"   - ëª¨ë¸ ì„¤ì •: Layers=2, Hidden=64, Embedding=256, Dropout=0.2")
    print(f"   - ëª¨ë¸ íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ ({model_size_mb:.2f} MB)")
    
    # [GPU í™œìš©ë„ ê°œì„ ] ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¦ê°€ (ì „ì²´ ë°ì´í„° ì‚¬ìš© ì‹œ)
    # ì „ì²´ ë°ì´í„°(53,000ê°œ) ì‚¬ìš© ì‹œ ë©”ëª¨ë¦¬ ì—¬ìœ  ìˆìœ¼ë©´ 128~256 ì¶”ì²œ
    batch_size = 128 if torch.cuda.is_available() else 64  # GPU ìˆìœ¼ë©´ 128, ì—†ìœ¼ë©´ 64
    train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_graphs, batch_size=batch_size, shuffle=False, num_workers=4)
    test_loader = DataLoader(test_graphs, batch_size=batch_size, shuffle=False, num_workers=4)
    print(f"   - Batch Size: {batch_size} (ì „ì²´ ë°ì´í„° ì‚¬ìš©, GPU í™œìš©ë„ ìµœëŒ€í™”)")
    print(f"   - Num Workers: 4 (ë°ì´í„° ë¡œë”© ì†ë„ í–¥ìƒ)")
    
    # [í•µì‹¬ ìˆ˜ì • 2] í•™ìŠµë¥  ë° Weight Decay ì¡°ì • (í•™ìŠµ ì´‰ì§„)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-5)  # 0.002 -> 0.005 (Loss ê°ì†Œ ì´‰ì§„)
    
    # Learning rate scheduler ì¶”ê°€ (í•™ìŠµ ì•ˆì •í™”, patience ì¦ê°€)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=15  # 10 -> 15 (ë„ˆë¬´ ë¹¨ë¦¬ ê°ì†Œí•˜ì§€ ì•Šë„ë¡)
    )
    print(f"   - Learning Rate: 0.005 (0.002 -> 0.005, Loss ê°ì†Œ ì´‰ì§„)")
    print(f"   - Weight Decay: 1e-5 (ë” ììœ ë¡œìš´ í•™ìŠµ)")
    print(f"   - LR Scheduler: ReduceLROnPlateau (patience=15)")
    
    # ---------------------------------------------------------
    # [ìˆ˜ì •] ì „ì²´ ë°ì´í„° ë¹„ìœ¨ì— ë§ì¶˜ ê°€ì¤‘ì¹˜ ìë™ ê³„ì‚°
    # ---------------------------------------------------------
    
    # Train setì—ì„œ í´ë˜ìŠ¤ ë¹„ìœ¨ ê³„ì‚°
    train_labels = [g.y.item() for g in train_graphs]
    n_train_pos = train_labels.count(1)
    n_train_neg = train_labels.count(0)
    train_ratio = n_train_neg / n_train_pos if n_train_pos > 0 else 1.0
    
    # ë¹„ìœ¨ì— ë§ì¶° ê°€ì¤‘ì¹˜ ê³„ì‚° (ê·¹ì‹¬í•œ ë¶ˆê· í˜• ëŒ€ì‘)
    # ì „ì²´ ë°ì´í„° ì‚¬ìš© ì‹œ ë¹„ìœ¨ì´ 30:1ì´ë¯€ë¡œ ë§¤ìš° ê°•ë ¥í•œ ê°€ì¤‘ì¹˜ í•„ìš”
    # ë¹„ìœ¨ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì œí•œ ì—†ìŒ) - 30:1ì´ë©´ 30.0 ì‚¬ìš©
    calculated_weight = train_ratio
    
    pos_weight = torch.tensor([calculated_weight]).to(device)
    
    print(f"   ğŸ”¥ [Full Data ì„¤ì •] Train Class Imbalance Ratio: 1:{train_ratio:.1f}")
    print(f"   ğŸ”¥ [Full Data ì„¤ì •] ì ìš©ëœ pos_weight: {pos_weight.item():.1f} (ë¹„ìœ¨ ê·¸ëŒ€ë¡œ, ì œí•œ ì—†ìŒ)")
    print(f"   âš ï¸  ê·¹ì‹¬í•œ ë¶ˆê· í˜• ëŒ€ì‘: ì¹˜ë§¤ í™˜ì í‹€ë¦¬ë©´ {pos_weight.item():.1f}ë°° ë²Œì !")
    
    # BCEWithLogitsLossê°€ í•™ìŠµ ì•ˆì •ì„±ì´ í›¨ì”¬ ì¢‹ìŒ
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    
    # í•™ìŠµ (ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ epoch ì¦ê°€)
    num_epochs = 100  # 50 -> 100 (AUCê°€ ê³„ì† ìƒìŠ¹ ì¤‘ì´ë¯€ë¡œ ë” í•™ìŠµ)
    best_auc = 0.0  # ëª…ì‹œì  ì´ˆê¸°í™”
    
    # Early stopping ì„¤ì • (ì™„í™”)
    patience = 30  # 20 -> 30 (ëª¨ë¸ì´ ì •ì‹  ì°¨ë¦¬ëŠ” ë° ì‹œê°„ í•„ìš”)
    patience_counter = 0
    best_loss = float('inf')
    
    print(f"\n   ğŸš€ í•™ìŠµ ì‹œì‘ (ìµœëŒ€ {num_epochs} epochs, patience={patience})")
    print(f"   ğŸ“Š Validation setìœ¼ë¡œ ëª¨ë¸ ì„ íƒ, Test setì€ ìµœì¢… í‰ê°€ì—ë§Œ ì‚¬ìš©")
    
    for epoch in range(num_epochs):
        train_loss, train_acc, train_auc, train_f1 = train_epoch(model, train_loader, optimizer, criterion, device)
        val_loss, val_acc, val_auc, val_f1 = evaluate(model, val_loader, criterion, device)
        
        # Learning rate scheduler ì—…ë°ì´íŠ¸ (Validation loss ì‚¬ìš©)
        scheduler.step(val_loss)
        
        # Best model ì €ì¥ (Validation AUC ê¸°ì¤€)
        if val_auc > best_auc:
            best_auc = val_auc
            torch.save(model.state_dict(), model_path)
            patience_counter = 0
            improved = "â­"
        else:
            patience_counter += 1
            improved = ""
        
        # Loss ê°œì„  ì²´í¬
        if val_loss < best_loss:
            best_loss = val_loss
        
        # ë§¤ epochë§ˆë‹¤ ì¶œë ¥ (í•™ìŠµ ëª¨ë‹ˆí„°ë§ ê°œì„ )
        current_lr = optimizer.param_groups[0]['lr']
        if (epoch + 1) % 1 == 0:  # ë§¤ epochë§ˆë‹¤ ì¶œë ¥
            print(f"   Epoch {epoch+1:3d}/{num_epochs}: "
                  f"Loss={train_loss:.4f}â†’{val_loss:.4f} | "
                  f"Train AUC={train_auc:.4f} | Val AUC={val_auc:.4f} (Best: {best_auc:.4f}) {improved} | "
                  f"Train F1={train_f1:.4f} | Val F1={val_f1:.4f} | LR={current_lr:.6f} | Patience: {patience_counter}/{patience}")
        
        # Early stopping (Validation AUC ê¸°ì¤€)
        if patience_counter >= patience:
            print(f"\n   âš ï¸  Early stopping at epoch {epoch+1} (Val AUC ê°œì„  ì—†ìŒ {patience} epochs)")
            break
    
    # ìµœì¢… í‰ê°€: Test setìœ¼ë¡œ í‰ê°€
    print(f"\n   ğŸ“Š ìµœì¢… í‰ê°€ (Test set)...")
    model.load_state_dict(torch.load(model_path))
    test_loss, test_acc, test_auc, test_f1 = evaluate(model, test_loader, criterion, device)
    
    print(f"\n   âœ… Best Val AUC: {best_auc:.4f}")
    print(f"   âœ… Final Test AUC: {test_auc:.4f}")
    print(f"   âœ… Final Test F1: {test_f1:.4f}")
    print(f"   âœ… ëª¨ë¸ ì €ì¥: {model_path}")
    
    print("\n" + "=" * 70)
    print("âœ… GNN ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    print("=" * 70)
    print("\nğŸ“ ì¶œë ¥ íŒŒì¼:")
    print(f"   - ëª¨ë¸: {model_path}")
    print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print("   - Explain & Subnetwork Detectionì€ ë³„ë„ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‹¤í–‰:")
    print("     python scripts/explain_gnn_subnet.py")


if __name__ == "__main__":
    main()
